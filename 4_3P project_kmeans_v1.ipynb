{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This assignment was completed by \n",
    "\n",
    "Bevan Fairleigh 219296864\n",
    "\n",
    "Rao Siddhant Yadav 220384895\n",
    "\n",
    "Chris Chong 220178171\n",
    "\n",
    "### for Deakin University SIT789 T2 2021\n",
    "### Applications of Computer Vision and Speech Processing\n",
    "### 4.3P Image Recognition System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE START BY LOOKING AT THE 4TH CELL\n",
    "\n",
    "## Set up\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "## any imports go here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228.0 532.0 760.0\n"
     ]
    }
   ],
   "source": [
    "## Get dataset\n",
    "\n",
    "## Assuming we can store up to 100MB on github, let's aim to keep dataset under 100MB\n",
    "## test data set loaded now, stolen from simpsons data set, \n",
    "## Most of this code was for loading in and presplitting the dataset.  ignore it now, unless we need it for a different set.\n",
    "\n",
    "\n",
    "# path = 'Dataset'\n",
    "# subpaths = [\"/test/\",\"/train/\",\"/validate/\"]\n",
    "\n",
    "# #Check files in dir\n",
    "\n",
    "# classes = os.listdir(path+subpaths[1])\n",
    "# # print(classes)\n",
    "\n",
    "# import random \n",
    "\n",
    "# filenames = [os.path.join(path+\"/classes/\"+classes[3], f) for f in os.listdir(path+\"/classes/\"+classes[3])]\n",
    "\n",
    "# random.shuffle(filenames)\n",
    "\n",
    "# total = len(filenames)\n",
    "# testsplit = .30 * total\n",
    "# trainsplit = (.4 * total) + testsplit\n",
    "# valsplit = (.3 * total) + trainsplit\n",
    "\n",
    "# count = 0\n",
    "# print (f\"{testsplit} {trainsplit} {valsplit}\")\n",
    "\n",
    "# for fn in filenames:\n",
    "#     if count < testsplit:\n",
    "#         os.rename(fn, \"Dataset/test/principal_skinner/\"+str(count)+\".jpg\")\n",
    "#     elif count < trainsplit:\n",
    "#         os.rename(fn, \"Dataset/train/principal_skinner/\"+str(count)+\".jpg\")\n",
    "#     else:\n",
    "#         os.rename(fn, \"Dataset/validate/principal_skinner/\"+str(count)+\".jpg\")\n",
    "    \n",
    "    \n",
    "#     count += 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'C:/Users/admin/Desktop/Masters AI/Applications of CV and SP/Week 4/Updated/Submissions/computervisionproject-main/dataset'\n",
    "# subpaths = [\"/test/\",\"/train/\",\"/validate/\"]\n",
    "# classes =  os.listdir(path+subpaths[1])\n",
    "\n",
    "# print(f\"Our classes are {classes}\")\n",
    "\n",
    "# training_file_names = []\n",
    "# test_file_names = []\n",
    "# val_file_names = []\n",
    "\n",
    "# training_labels = []\n",
    "# test_labels = []\n",
    "# val_labels = []\n",
    "\n",
    "# for i in range(0,len(classes)):\n",
    "#     sub_path = path + \"/train/\" + classes[i] + '/'\n",
    "#     sub_file_names = [os.path.join(sub_path, f) for f in os.listdir(sub_path)]\n",
    "#     sub_labels = [i] * len(sub_file_names)\n",
    "#     training_file_names += sub_file_names\n",
    "#     training_labels += sub_labels\n",
    "    \n",
    "#     sub_path = path + \"/test/\" + classes[i] + '/'\n",
    "#     sub_file_names = [os.path.join(sub_path, f) for f in os.listdir(sub_path)]\n",
    "#     sub_labels = [i] * len(sub_file_names)\n",
    "#     test_file_names += sub_file_names\n",
    "#     test_labels += sub_labels\n",
    "    \n",
    "#     sub_path = path + \"/validate/\" + classes[i] + '/'\n",
    "#     sub_file_names = [os.path.join(sub_path, f) for f in os.listdir(sub_path)]\n",
    "#     sub_labels = [i] * len(sub_file_names)\n",
    "#     val_file_names += sub_file_names\n",
    "#     val_labels += sub_labels\n",
    "    \n",
    "# print(training_labels)\n",
    "# print(test_labels)\n",
    "# print(val_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Starting here from 4.1\n",
    "# # 1. Bag-of-Words (BoW) model\n",
    "\n",
    "# import numpy as np\n",
    "# import cv2 as cv\n",
    "# from sklearn.cluster import KMeans\n",
    "# class Dictionary(object):\n",
    "#     def __init__(self, name, img_filenames, num_words):\n",
    "#         self.name = name #name of your dictionary\n",
    "#         self.img_filenames = img_filenames #list of image filenames\n",
    "#         self.num_words = num_words #the number of words\n",
    "        \n",
    "#         self.training_data = [] #this is the training data required by the K-Means algorithm\n",
    "#         self.words = [] #list of words, which are the centroids of clusters\n",
    " \n",
    "#     def learn(self):\n",
    "#         sift = cv.xfeatures2d.SIFT_create()\n",
    "        \n",
    "#         num_keypoints = [] #this is used to store the number of keypoints in each image\n",
    " \n",
    "#         #load training images and compute SIFT descriptors\n",
    "#         for filename in self.img_filenames:\n",
    "#             img = cv.imread(filename)\n",
    "#             img_gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "#             list_des = sift.detectAndCompute(img_gray, None)[1]\n",
    "#             if list_des is None:\n",
    "#                 num_keypoints.append(0)\n",
    "#             else:\n",
    "#                 num_keypoints.append(len(list_des))\n",
    "#                 for des in list_des:\n",
    "#                     self.training_data.append(des)\n",
    " \n",
    "#         #cluster SIFT descriptors using K-means algorithm\n",
    "#         kmeans = KMeans(self.num_words) #num_words is number of clusters\n",
    "#         kmeans.fit(self.training_data)\n",
    "#         self.words = kmeans.cluster_centers_\n",
    " \n",
    "#         #create word histograms for training images\n",
    "#         training_word_histograms = [] #list of word histograms of all training images\n",
    "#         index = 0\n",
    "#         for i in range(0, len(self.img_filenames)):\n",
    "#             #for each file, create a histogram\n",
    "#             histogram = np.zeros(self.num_words, np.float32)\n",
    "#             #if some keypoints exist\n",
    "#             if num_keypoints[i] > 0:\n",
    "#                 for j in range(0, num_keypoints[i]):\n",
    "#                     histogram[kmeans.labels_[j + index]] += 1\n",
    "#                 index += num_keypoints[i]\n",
    "#                 histogram /= num_keypoints[i]\n",
    "#                 training_word_histograms.append(histogram)\n",
    " \n",
    "#         return training_word_histograms\n",
    " \n",
    "#     def create_word_histograms(self, img_filenames):\n",
    "#         sift = cv.xfeatures2d.SIFT_create()\n",
    "#         histograms = []\n",
    "        \n",
    "#         for filename in img_filenames:\n",
    "#             img = cv.imread(filename)\n",
    "#             img_gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "#             descriptors = sift.detectAndCompute(img_gray, None)[1]\n",
    "            \n",
    "#             histogram = np.zeros(self.num_words, np.float32) #word histogram for the input image\n",
    "            \n",
    "#             if descriptors is not None:\n",
    "#                 for des in descriptors:\n",
    "#                     #find the best matching word\n",
    "#                     min_distance = 1111111 #this can be any large number\n",
    "#                     matching_word_ID = -1 #initial matching_word_ID=-1 means no matching\n",
    "                    \n",
    "#                     for i in range(0, self.num_words): #search for the best matching word\n",
    "#                         distance = np.linalg.norm(des - self.words[i])\n",
    "#                         if distance < min_distance:\n",
    "#                             min_distance = distance\n",
    "#                             matching_word_ID = i\n",
    " \n",
    "#                     histogram[matching_word_ID] += 1\n",
    " \n",
    "#                 histogram /= len(descriptors) #normalise histogram to frequencies\n",
    " \n",
    "#             histograms.append(histogram)\n",
    "    \n",
    "#         return histograms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset/Train/chief_wiggum/316.jpg', 'dataset/Train/chief_wiggum/317.jpg', 'dataset/Train/chief_wiggum/318.jpg', 'dataset/Train/chief_wiggum/319.jpg', 'dataset/Train/chief_wiggum/320.jpg', 'dataset/Train/chief_wiggum/321.jpg', 'dataset/Train/chief_wiggum/322.jpg', 'dataset/Train/chief_wiggum/323.jpg', 'dataset/Train/chief_wiggum/324.jpg', 'dataset/Train/chief_wiggum/325.jpg', 'dataset/Train/chief_wiggum/326.jpg', 'dataset/Train/chief_wiggum/327.jpg', 'dataset/Train/chief_wiggum/328.jpg', 'dataset/Train/chief_wiggum/329.jpg', 'dataset/Train/chief_wiggum/330.jpg', 'dataset/Train/chief_wiggum/331.jpg', 'dataset/Train/chief_wiggum/332.jpg', 'dataset/Train/chief_wiggum/333.jpg', 'dataset/Train/chief_wiggum/334.jpg', 'dataset/Train/chief_wiggum/335.jpg', 'dataset/Train/chief_wiggum/336.jpg', 'dataset/Train/chief_wiggum/337.jpg', 'dataset/Train/chief_wiggum/338.jpg', 'dataset/Train/chief_wiggum/339.jpg', 'dataset/Train/chief_wiggum/340.jpg', 'dataset/Train/chief_wiggum/341.jpg', 'dataset/Train/chief_wiggum/342.jpg', 'dataset/Train/chief_wiggum/343.jpg', 'dataset/Train/chief_wiggum/344.jpg', 'dataset/Train/chief_wiggum/345.jpg', 'dataset/Train/chief_wiggum/346.jpg', 'dataset/Train/chief_wiggum/347.jpg', 'dataset/Train/chief_wiggum/348.jpg', 'dataset/Train/chief_wiggum/349.jpg', 'dataset/Train/chief_wiggum/350.jpg', 'dataset/Train/chief_wiggum/351.jpg', 'dataset/Train/chief_wiggum/352.jpg', 'dataset/Train/chief_wiggum/353.jpg', 'dataset/Train/chief_wiggum/354.jpg', 'dataset/Train/chief_wiggum/355.jpg', 'dataset/Train/comic_book_guy/172.jpg', 'dataset/Train/comic_book_guy/173.jpg', 'dataset/Train/comic_book_guy/174.jpg', 'dataset/Train/comic_book_guy/175.jpg', 'dataset/Train/comic_book_guy/176.jpg', 'dataset/Train/comic_book_guy/177.jpg', 'dataset/Train/comic_book_guy/178.jpg', 'dataset/Train/comic_book_guy/179.jpg', 'dataset/Train/comic_book_guy/180.jpg', 'dataset/Train/comic_book_guy/181.jpg', 'dataset/Train/comic_book_guy/182.jpg', 'dataset/Train/comic_book_guy/183.jpg', 'dataset/Train/comic_book_guy/184.jpg', 'dataset/Train/comic_book_guy/185.jpg', 'dataset/Train/comic_book_guy/186.jpg', 'dataset/Train/comic_book_guy/187.jpg', 'dataset/Train/comic_book_guy/188.jpg', 'dataset/Train/comic_book_guy/189.jpg', 'dataset/Train/comic_book_guy/190.jpg', 'dataset/Train/comic_book_guy/191.jpg', 'dataset/Train/comic_book_guy/192.jpg', 'dataset/Train/comic_book_guy/193.jpg', 'dataset/Train/comic_book_guy/194.jpg', 'dataset/Train/comic_book_guy/195.jpg', 'dataset/Train/comic_book_guy/196.jpg', 'dataset/Train/comic_book_guy/197.jpg', 'dataset/Train/comic_book_guy/198.jpg', 'dataset/Train/comic_book_guy/199.jpg', 'dataset/Train/comic_book_guy/200.jpg', 'dataset/Train/comic_book_guy/201.jpg', 'dataset/Train/comic_book_guy/202.jpg', 'dataset/Train/comic_book_guy/203.jpg', 'dataset/Train/comic_book_guy/204.jpg', 'dataset/Train/comic_book_guy/205.jpg', 'dataset/Train/comic_book_guy/206.jpg', 'dataset/Train/comic_book_guy/207.jpg', 'dataset/Train/comic_book_guy/208.jpg', 'dataset/Train/comic_book_guy/209.jpg', 'dataset/Train/comic_book_guy/210.jpg', 'dataset/Train/comic_book_guy/211.jpg', 'dataset/Train/mayor_quimby/121.jpg', 'dataset/Train/mayor_quimby/122.jpg', 'dataset/Train/mayor_quimby/123.jpg', 'dataset/Train/mayor_quimby/124.jpg', 'dataset/Train/mayor_quimby/125.jpg', 'dataset/Train/mayor_quimby/126.jpg', 'dataset/Train/mayor_quimby/127.jpg', 'dataset/Train/mayor_quimby/128.jpg', 'dataset/Train/mayor_quimby/129.jpg', 'dataset/Train/mayor_quimby/130.jpg', 'dataset/Train/mayor_quimby/131.jpg', 'dataset/Train/mayor_quimby/132.jpg', 'dataset/Train/mayor_quimby/133.jpg', 'dataset/Train/mayor_quimby/134.jpg', 'dataset/Train/mayor_quimby/135.jpg', 'dataset/Train/mayor_quimby/136.jpg', 'dataset/Train/mayor_quimby/137.jpg', 'dataset/Train/mayor_quimby/138.jpg', 'dataset/Train/mayor_quimby/139.jpg', 'dataset/Train/mayor_quimby/140.jpg', 'dataset/Train/mayor_quimby/141.jpg', 'dataset/Train/mayor_quimby/142.jpg', 'dataset/Train/mayor_quimby/143.jpg', 'dataset/Train/mayor_quimby/144.jpg', 'dataset/Train/mayor_quimby/145.jpg', 'dataset/Train/mayor_quimby/146.jpg', 'dataset/Train/mayor_quimby/147.jpg', 'dataset/Train/mayor_quimby/148.jpg', 'dataset/Train/mayor_quimby/149.jpg', 'dataset/Train/mayor_quimby/150.jpg', 'dataset/Train/mayor_quimby/151.jpg', 'dataset/Train/mayor_quimby/152.jpg', 'dataset/Train/mayor_quimby/153.jpg', 'dataset/Train/mayor_quimby/154.jpg', 'dataset/Train/mayor_quimby/155.jpg', 'dataset/Train/mayor_quimby/156.jpg', 'dataset/Train/mayor_quimby/157.jpg', 'dataset/Train/mayor_quimby/158.jpg', 'dataset/Train/mayor_quimby/159.jpg', 'dataset/Train/mayor_quimby/160.jpg', 'dataset/Train/principal_skinner/228.jpg', 'dataset/Train/principal_skinner/229.jpg', 'dataset/Train/principal_skinner/230.jpg', 'dataset/Train/principal_skinner/231.jpg', 'dataset/Train/principal_skinner/232.jpg', 'dataset/Train/principal_skinner/233.jpg', 'dataset/Train/principal_skinner/234.jpg', 'dataset/Train/principal_skinner/235.jpg', 'dataset/Train/principal_skinner/236.jpg', 'dataset/Train/principal_skinner/237.jpg', 'dataset/Train/principal_skinner/238.jpg', 'dataset/Train/principal_skinner/239.jpg', 'dataset/Train/principal_skinner/240.jpg', 'dataset/Train/principal_skinner/241.jpg', 'dataset/Train/principal_skinner/242.jpg', 'dataset/Train/principal_skinner/243.jpg', 'dataset/Train/principal_skinner/244.jpg', 'dataset/Train/principal_skinner/245.jpg', 'dataset/Train/principal_skinner/246.jpg', 'dataset/Train/principal_skinner/247.jpg', 'dataset/Train/principal_skinner/248.jpg', 'dataset/Train/principal_skinner/249.jpg', 'dataset/Train/principal_skinner/250.jpg', 'dataset/Train/principal_skinner/251.jpg', 'dataset/Train/principal_skinner/252.jpg', 'dataset/Train/principal_skinner/253.jpg', 'dataset/Train/principal_skinner/254.jpg', 'dataset/Train/principal_skinner/255.jpg', 'dataset/Train/principal_skinner/256.jpg', 'dataset/Train/principal_skinner/257.jpg', 'dataset/Train/principal_skinner/258.jpg', 'dataset/Train/principal_skinner/259.jpg', 'dataset/Train/principal_skinner/260.jpg', 'dataset/Train/principal_skinner/261.jpg', 'dataset/Train/principal_skinner/262.jpg', 'dataset/Train/principal_skinner/263.jpg', 'dataset/Train/principal_skinner/264.jpg', 'dataset/Train/principal_skinner/265.jpg', 'dataset/Train/principal_skinner/266.jpg', 'dataset/Train/principal_skinner/267.jpg']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "# Creating list of simpson character labels of all training images\n",
    "\n",
    "import os\n",
    "simpson_char = ['chief_wiggum', 'comic_book_guy', 'mayor_quimby','principal_skinner']\n",
    "path = 'dataset/'\n",
    "training_file_names = []\n",
    "training_simpson_char_labels = []\n",
    "for i in range(0, len(simpson_char)):\n",
    "    sub_path = path + 'Train/' + simpson_char[i] + '/'\n",
    "    sub_file_names = [os.path.join(sub_path, f) for f in os.listdir(sub_path)]\n",
    "    sub_simpson_char_labels = [i] * len(sub_file_names) #create a list of N elements, all are i\n",
    "    training_file_names += sub_file_names\n",
    "    training_simpson_char_labels += sub_simpson_char_labels\n",
    "\n",
    "print(training_file_names)\n",
    "print(training_simpson_char_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(5,5, figsize=(10,10))\n",
    "num_rows = np.shape(training_file_names)[0]\n",
    "\n",
    "images = []\n",
    "img_num = []\n",
    "\n",
    "for i in range(25):\n",
    "    img_num.append(random.randint(0,num_rows-1))\n",
    "    sub_path = path + 'Train/' + simpson_char[i] + '/'\n",
    "    sub_file_names = [os.path.join(sub_path, f) for f in os.listdir(sub_path)]\n",
    "    sub_simpson_char_labels = [i] * len(sub_file_names) #create a list of N elements, all are i\n",
    "    training_file_names += sub_file_names\n",
    "    training_simpson_char_labels += sub_simpson_char_labels\n",
    "    plt\n",
    "\n",
    "\n",
    "for i in range(25):\n",
    "  img_num.append(random.randint(0,num_rows-1))\n",
    "  images.append(x_train[img_num[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building and Image Recognition System\n",
    "### 2.1 Building BoW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building an Image Recognition System\n",
    "\n",
    "\n",
    "sift = cv.xfeatures2d.SIFT_create()\n",
    "num_keypoints = [] #this is used to store the number of keypoints in each image\n",
    "training_data = []\n",
    " \n",
    "#load training images and compute SIFT descriptors\n",
    "for filename in training_file_names:\n",
    "    img = cv.imread(filename)\n",
    "    img_gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    list_des = sift.detectAndCompute(img_gray, None)[1]\n",
    "    if list_des is None:\n",
    "        num_keypoints.append(0)\n",
    "    else:\n",
    "        num_keypoints.append(len(list_des))\n",
    "        for des in list_des:\n",
    "            training_data.append(des)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow method to Determine K\n",
    "distortions = []\n",
    "inertias = []\n",
    "mapping1 = {}\n",
    "mapping2 = {}\n",
    "K = range(10, 100)\n",
    "\n",
    "# timer\n",
    "start_time = time.time()\n",
    "for k in K:\n",
    "    # Building and fitting the model\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(training_data)\n",
    "    kmeanModel.fit(training_data)\n",
    " \n",
    "    distortions.append(sum(np.min(cdist(training_data, kmeanModel.cluster_centers_,\n",
    "                                        'euclidean'), axis=1)) / np.concatenate( training_data, axis=0 ).shape[0])\n",
    "    inertias.append(kmeanModel.inertia_)\n",
    " \n",
    "    mapping1[k] = sum(np.min(cdist(training_data, kmeanModel.cluster_centers_,\n",
    "                                   'euclidean'), axis=1)) / np.concatenate( training_data, axis=0 ).shape[0]\n",
    "    mapping2[k] = kmeanModel.inertia_\n",
    "# end time\n",
    "end_time = time.time()\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('Values of K')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method using Distortion')\n",
    "plt.show()\n",
    "\n",
    "print('run time =',(end_time-start_time)/60,'minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: ONCE WE GET THE BEST K FROM HERE, WE CAN SIMPLY RUN 4.1 FOR THAT K VALUE BY REPLACING num_words\n",
    "\n",
    "\n",
    "# we now have a built dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Building Classifiers\n",
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the validation set file names and corresponding labels\n",
    "\n",
    "# array to store the image paths\n",
    "validate_file_names = []\n",
    "# array for true labels based on the folder name\n",
    "validate_simpson_char_labels = []\n",
    "\n",
    "for subdir, dirs, files in os.walk('dataset/validate/'):\n",
    "    for file in files:\n",
    "        filepath = subdir+'/'+file\n",
    "        test_file_names.append(filepath)\n",
    "        if subdir == 'dataset/validate/chief_wiggum':\n",
    "            validate_simpson_char_labels.append(0)\n",
    "        if subdir == 'dataset/validate/comic_book_guy':\n",
    "            validate_simpson_char_labels.append(1)\n",
    "        if subdir == 'dataset/validate/mayor_quimby':\n",
    "            validate_simpson_char_labels.append(2)\n",
    "        if subdir == 'dataset/validate/principal_skinner':\n",
    "            validate_simpson_char_labels.append(3)\n",
    "\n",
    "# extract the word histogram for the images\n",
    "word_histograms = dictionary.create_word_histograms(validate_file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 k-NN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through different numbers of nearest neighbours\n",
    "num_nearest_neighbours = [5, 10, 15, 20, 25, 30]\n",
    "\n",
    "for n in num_nearest_neighbours:\n",
    "    knn = KNeighborsClassifier(n_neighbors = n)\n",
    "    # Train on TRAINING set\n",
    "    knn.fit(training_word_histograms, training_simpson_char_labels)\n",
    "    predicted_simpson_char_labels = knn.predict(word_histograms)\n",
    "    print(n,'nearest neighbours:')\n",
    "    print(' Accuracy :~',100*np.round(accuracy_score(training_simpson_char_labels, predicted_simpson_char_labels),5),'%')\n",
    "    # Tune on VALIDATION set\n",
    "    cm = confusion_matrix(validate_simpson_char_labels, predicted_simpson_char_labels)\n",
    "    print(' Confusion Matrix')\n",
    "    print('',cm,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Linear SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through different C values\n",
    "C = [0.1, 1, 10, 100]\n",
    "\n",
    "for i in C:\n",
    "    svm_classifier = svm.SVC(C = i, kernel = 'linear')\n",
    "    # Train on TRAINING set\n",
    "    svm_classifier.fit(training_word_histograms, training_simpson_char_labels)\n",
    "    predicted_simpson_char_labels = svm_classifier.predict(word_histograms)\n",
    "    print('C value =', i,':')\n",
    "    print(' Accuracy :~',100*np.round(accuracy_score(training_simpson_char_labels, predicted_simpson_char_labels),5),'%')\n",
    "    # Tune on VALIDATION set\n",
    "    cm = confusion_matrix(validate_simpson_char_labels, predicted_simpson_char_labels)\n",
    "    print(' Confusion Matrix')\n",
    "    print('',cm,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through different n_estimators\n",
    "n_estimators = [50, 100, 150, 200, 250]\n",
    "\n",
    "for i in n_estimators:\n",
    "    adb_classifier = AdaBoostClassifier(n_estimators = i, #weak classifiers\n",
    "                                        random_state = 0)\n",
    "    # Train on TRAINING set\n",
    "    adb_classifier.fit(training_word_histograms, training_simpson_char_labels)\n",
    "    predicted_simpson_char_labels = adb_classifier.predict(word_histograms)\n",
    "    print('Number of estimators =', i,':')\n",
    "    print(' Accuracy :~',100*np.round(accuracy_score(training_simpson_char_labels, predicted_simpson_char_labels),5),'%')\n",
    "    # Tune on VALIDATION set\n",
    "    cm = confusion_matrix(validate_simpson_char_labels, predicted_simpson_char_labels)\n",
    "    print(' Confusion Matrix')\n",
    "    print('',cm,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Evaluating Classifiers\n",
    "### 2.2.1 Comparison and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the test set file names and corresponding labels\n",
    "\n",
    "# array to store the image paths\n",
    "test_file_names = []\n",
    "# array for true labels based on the folder name\n",
    "test_simpson_char_labels = []\n",
    "\n",
    "for subdir, dirs, files in os.walk('dataset/validate/'):\n",
    "    for file in files:\n",
    "        filepath = subdir+'/'+file\n",
    "        test_file_names.append(filepath)\n",
    "        if subdir == 'dataset/test/chief_wiggum':\n",
    "            test_simpson_char_labels.append(0)\n",
    "        if subdir == 'dataset/test/comic_book_guy':\n",
    "            test_simpson_char_labels.append(1)\n",
    "        if subdir == 'dataset/test/mayor_quimby':\n",
    "            test_simpson_char_labels.append(2)\n",
    "        if subdir == 'dataset/test/principal_skinner':\n",
    "            test_simpson_char_labels.append(3)\n",
    "\n",
    "# extract the word histogram for the images\n",
    "word_histograms = dictionary.create_word_histograms(test_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all the three tuned models on the TEST set\n",
    "\n",
    "# k-NN\n",
    "\n",
    "\n",
    "# SVM\n",
    "\n",
    "\n",
    "# AdaBoost\n",
    "\n",
    "\n",
    "# Recognition Accuracy\n",
    "acc = accuracy_score(test_Y, pred_y)\n",
    "acc2 = accuracy_score(test_Y2, pred_y)\n",
    "acc3 = accuracy_score(test_Y3, pred_y)\n",
    "\n",
    "# Confusion Matrices\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "cm = plot_confusion_matrix(classifier, test_x,  test_Y, values_format =\"\",cmap=\"Blues\", ax=ax)\n",
    "ax.set_title('k-NN Classifier Confusion Matrix')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "cm = plot_confusion_matrix(classifier, test_x,  test_Y2, values_format =\"\",cmap=\"Blues\", ax=ax)\n",
    "ax.set_title('Linear SVM Classifier Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "cm = plot_confusion_matrix(classifier, test_x,  test_Y3, values_format =\"\",cmap=\"Blues\", ax=ax)\n",
    "ax.set_title('AdaBoost Classifier Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Classifier   | Tuned Hyperparameter Value  | Recognition Accuracy (%)|\n",
    "|---|---|---|\n",
    "| k-NN  | num_nearest_neighbours =   | 80.00%  |\n",
    "| Linear SVM  | C =   | 80.00%  |\n",
    "| AdaBoost  | n_estimators =   | 80.00%  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
